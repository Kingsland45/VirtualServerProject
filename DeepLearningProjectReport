Austin Kingsland
MUID: kingsla
Deep learning Project Report

To enhance the performance of a Convolutional Neural Network (CNN) on the
CIFAR-10 dataset, several substantial modifications must be made to both the model
architecture and training parameters. The initial model implemented, YourNet,
comprised five convolutional layers with kernel sizes of 11x11, 7x7, and 5x5,
respectively. These layers were followed by two pooling layers and two fully connected
layers, with the final fully connected layer projecting onto the 10 output classes. While
effective to a degree, this architecture lacked advanced regularization techniques like
batch normalization and dropout, which are essential for improving generalization and
convergence speed in deep learning models. Additionally, the optimizer used was
Stochastic Gradient Descent (SGD) with momentum, which though powerful, required
meticulous tuning of learning rates and did not include a learning rate scheduler.
In response to these limitations, EnhancedNet was introduced as a more robust
alternative. This new model architecture incorporated batch normalization layers after
each convolutional layer to stabilize and accelerate training. Specifically, batch
normalization was applied to layers with 32, 64, 128, 256, and 256 filters, respectively.
Dropout layers were also integrated after each pooling layer with dropout rates of 0.25
and an additional dropout layer with a rate of 0.5 before the final fully connected layer.
These dropout layers played a crucial role in preventing overfitting by randomly
dropping units during training. The pooling strategy included three MaxPooling layers to
progressively reduce the spatial dimensions, enhancing the network's ability to capture
hierarchical features.
EnhancedNet increased the complexity of the convolutional layers by using a
greater number of filters, starting with 32 filters in the first layer and scaling up to 256
filters in the deeper layers. The kernel size was uniformly set to 3x3, which is a common
choice in modern architectures due to its effectiveness in capturing fine-grained
features. The network architecture also included two fully connected layers, where the
first fully connected layer had 512 units, followed by a final fully connected layer
projecting to the 10 output classes. This configuration allowed for a more expressive
model capable of capturing complex patterns in the CIFAR-10 dataset.
The optimization strategy was also significantly improved. The optimizer was
switched from SGD with momentum to Adam, which is known for its adaptive learning
rate capabilities and generally provides better convergence with less parameter tuning.
An initial learning rate of 0.001 was chosen for Adam. Additionally, a cosine annealing
learning rate scheduler with T_max set to 50 was employed to dynamically adjust the
learning rate during training, allowing for smoother convergence and helping to avoid
local minima. This learning rate scheduler reduced the learning rate following a cosine
curve over the training epochs, which facilitated a more efficient exploration of the
parameter space.
To further enhance model performance, more aggressive data augmentation
techniques were applied. These included random cropping with padding of 4 pixels,
random horizontal flipping, random rotation up to 10 degrees, and color jittering with
brightness, contrast, saturation, and hue adjustments. These augmentations helped in
creating a more diverse training set, making the model more robust to variations in the
input data. The data was normalized using the mean and standard deviation of the
CIFAR-10 dataset ((0.4914, 0.4822, 0.4465) and (0.2023, 0.1994, 0.2010),
respectively). Collectively, these enhancements led to a significant improvement in
model accuracy, achieving 88% on the test set after 50 epochs, compared to the lower
performance of the original model, which plateaued at around 78% by epoch 10 and
struggled to exceed 86% in later epochs.
Throughout this process, several valuable lessons were learned. The importance
of regularization techniques, such as batch normalization and dropout, became evident
as they significantly improved the model's stability and generalization capabilities. The
switch to the Adam optimizer highlighted the benefits of adaptive learning rates,
especially when combined with a learning rate scheduler like cosine annealing.
Additionally, the impact of data augmentation on model robustness was underscored,
demonstrating how synthetic variations in the training data can lead to better
performance on unseen data. These experiences have deepened my understanding of
advanced neural network architectures and optimization strategies, providing a solid foundation for machine learning and deep learning.
